// SPDX-License-Identifier: Apache-2.0 OR MIT OR Unlicense

// Single pass prefix sum without barriers.

#version 450

#extension GL_KHR_memory_scope_semantics : enable

//Tuning paramters
#define VEC4            4
#define LG_WG_SIZE      9
#define WG_SIZE         (1 << LG_WG_SIZE)
#define PART_SIZE       (WG_SIZE * VEC4)

//For the decoupled lookback
#define FLAG_NOT_READY  0    
#define FLAG_REDUCTION  1
#define FLAG_INCLUSIVE  2
#define FLAG_MASK       3

layout(local_size_x = WG_SIZE, local_size_y = 1) in;

layout(set = 0, binding = 0) readonly buffer InBuf {
    uvec4[] inbuf;
};

layout(set = 0, binding = 1) buffer OutBuf {
    uvec4[] outbuf;
};

layout(set = 0, binding = 2) coherent buffer StateBuf {
    uint part_counter;
    uint[] state;
};

shared uint s_broadcast;
shared uint s_reduce[WG_SIZE];

void main() {
    //acquire the partition index
    //set the lock
    if(gl_LocalInvocationID.x == 0){
        s_broadcast = atomicAdd(part_counter, 1);
    }
    barrier();
    uint part_id = s_broadcast;

    uint red = 0;
    uvec4 t_scan[VEC4];
    uint threadOffset = gl_LocalInvocationID.x * VEC4 + part_id * PART_SIZE;
    for(uint i = 0; i < VEC4; ++i){
        t_scan[i] = inbuf[i + threadOffset];
        t_scan[i].x += red;
        t_scan[i].y += t_scan[i].x;
        t_scan[i].z += t_scan[i].y;
        t_scan[i].w += t_scan[i].z;
        red = t_scan[i].w;
    }

    s_reduce[gl_LocalInvocationID.x] = red;
    for (uint i = 0; i < LG_WG_SIZE; i++) {
        barrier();
        if (gl_LocalInvocationID.x >= (1u << i)) {
            red += s_reduce[gl_LocalInvocationID.x - (1u << i)];
        }
        barrier();
        s_reduce[gl_LocalInvocationID.x] = red;
    }
    
    if(gl_LocalInvocationID.x == WG_SIZE - 1){
        atomicStore(state[part_id], red << 2 | (part_id != 0 ? FLAG_REDUCTION : FLAG_INCLUSIVE), gl_ScopeDevice, 0, 0);
    }

    if(part_id != 0){
        if(gl_LocalInvocationID.x == WG_SIZE - 1){
            uint prev_reduction = 0;
            uint lookback_id = part_id - 1;
            while(true){
                uint flag_payload = atomicLoad(state[lookback_id], gl_ScopeDevice, 0, 0);
                if((flag_payload & FLAG_MASK) == FLAG_INCLUSIVE){
                    prev_reduction += flag_payload >> 2;
                    atomicStore(state[part_id], red + prev_reduction << 2 | FLAG_INCLUSIVE, gl_ScopeDevice, 0, 0);
                    s_broadcast = prev_reduction;
                    break;
                }

                if((flag_payload & FLAG_MASK) == FLAG_REDUCTION){
                    prev_reduction += flag_payload >> 2;
                    lookback_id--;
                }
            }
        }
    }
    barrier();

    uint prev = (gl_LocalInvocationID.x != 0 ? s_reduce[gl_LocalInvocationID.x - 1] : 0) + s_broadcast;
    for(uint i = 0; i < VEC4; ++i){
        outbuf[i + threadOffset] = t_scan[i] + prev;
    }
}